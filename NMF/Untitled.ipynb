{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce11fbd0-cf36-44cf-8b9f-139fc1aaa833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NMF by alternative non-negative least squares using projected gradients\n",
    "# Author: Chih-Jen Lin, National Taiwan University\n",
    "# Python/numpy translation: Anthony Di Franco\n",
    "\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from time import time\n",
    "from sys import stdout\n",
    "\n",
    "def nmf(V,Winit,Hinit,tol,timelimit,maxiter):\n",
    "    \"\"\"\n",
    "    (W,H) = nmf(V,Winit,Hinit,tol,timelimit,maxiter)\n",
    "    W,H: output solution\n",
    "    Winit,Hinit: initial solution\n",
    "    tol: tolerance for a relative stopping condition\n",
    "    timelimit, maxiter: limit of time and iterations\n",
    "    \"\"\"\n",
    "\n",
    "    W = Winit; H = Hinit; initt = time();\n",
    "\n",
    "    gradW = dot(W, dot(H, H.T)) - dot(V, H.T)\n",
    "    gradH = dot(dot(W.T, W), H) - dot(W.T, V)\n",
    "    initgrad = norm(r_[gradW, gradH.T])\n",
    "    print ('Init gradient norm %f' % initgrad )\n",
    "    tolW = max(0.001,tol)*initgrad\n",
    "    tolH = tolW\n",
    "\n",
    "    for iter in range(1,maxiter):\n",
    "    # stopping condition\n",
    "        projnorm = norm(r_[gradW[logical_or(gradW<0, W>0)],\n",
    "                                     gradH[logical_or(gradH<0, H>0)]])\n",
    "        if projnorm < tol*initgrad or time() - initt > timelimit: break\n",
    "\n",
    "        (W, gradW, iterW) = nlssubprob(V.T,H.T,W.T,tolW,1000)\n",
    "        W = W.T\n",
    "        gradW = gradW.T\n",
    "\n",
    "        if iterW==1: tolW = 0.1 * tolW\n",
    "\n",
    "        (H,gradH,iterH) = nlssubprob(V,W,H,tolH,1000)\n",
    "        if iterH==1: tolH = 0.1 * tolH\n",
    "\n",
    "        if iter % 10 == 0: stdout.write('.')\n",
    "\n",
    "    print ('\\nIter = %d Final proj-grad norm %f' % (iter, projnorm))\n",
    "    return (W,H)\n",
    "\n",
    "def nlssubprob(V,W,Hinit,tol,maxiter):\n",
    "    \"\"\"\n",
    "    H, grad: output solution and gradient\n",
    "    iter: #iterations used\n",
    "    V, W: constant matrices\n",
    "    Hinit: initial solution\n",
    "    tol: stopping tolerance\n",
    "    maxiter: limit of iterations\n",
    "    \"\"\"\n",
    "\n",
    "    H = Hinit\n",
    "    WtV = dot(W.T, V)\n",
    "    WtW = dot(W.T, W) \n",
    "\n",
    "    alpha = 1; beta = 0.1;\n",
    "    for iter in range(1, maxiter):  \n",
    "        grad = dot(WtW, H) - WtV\n",
    "        projgrad = norm(grad[logical_or(grad < 0, H >0)])\n",
    "        if projgrad < tol: break\n",
    "\n",
    "    # search step size \n",
    "    for inner_iter in range(1,20):\n",
    "        Hn = H - alpha*grad\n",
    "        Hn = where(Hn > 0, Hn, 0)\n",
    "        d = Hn-H\n",
    "        gradd = sum(grad * d)\n",
    "        dQd = sum(dot(WtW,d) * d)\n",
    "        suff_decr = 0.99*gradd + 0.5*dQd < 0;\n",
    "        if inner_iter == 1:\n",
    "            decr_alpha = not suff_decr; Hp = H;\n",
    "        if decr_alpha: \n",
    "            if suff_decr:\n",
    "                H = Hn; break;\n",
    "            else:\n",
    "                alpha = alpha * beta;\n",
    "        else:\n",
    "            if not suff_decr or (Hp == Hn).all():\n",
    "                H = Hp; break;\n",
    "            else:\n",
    "                alpha = alpha/beta; Hp = Hn;\n",
    "\n",
    "    if iter == maxiter:\n",
    "        print ('Max iter in nlssubprob')\n",
    "    return (H, grad, iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0e935ac-3ce5-4280-8d00-2c44238ab81a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v = array([[0.1,0.2],[0.3,0.4]])\n",
    "w = array([[0.1,0.1],[0.1,0.1]])\n",
    "h = array([[0.2,0.2],[0.3,0.3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5168666a-aea1-4a2e-8608-ba543f14430c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init gradient norm 0.242487\n",
      ".......\n",
      "Iter = 72 Final proj-grad norm 0.000000\n"
     ]
    }
   ],
   "source": [
    "w_new, h_new = nmf(v,w,h,0.000000001,1000,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56bf24d6-3f65-40fb-92c4-6c35161c7ad0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.2],\n",
       "       [0.3, 0.4]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(w_new,h_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b759537-a7fc-48a2-a693-5f192f073781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
